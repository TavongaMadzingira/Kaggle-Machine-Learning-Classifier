# -*- coding: utf-8 -*-
"""CODE 181485.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jMbf87lHam3L9vmniJuhlWwhi-3Zw-vT

please run the following blocks in order to access kaggle datasets
"""

!pip install kaggle

!mkdir .kaggle

import json
token = {"username":"tavongamadzingira","key":"2047942449698eb96df9954ba327b14c"}
with open('/content/.kaggle/kaggle.json', 'w') as file:
    json.dump(token, file)

!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json

!kaggle config set -n path -v{/content}

!chmod 600 /root/.kaggle/kaggle.json

!kaggle competitions download -c bring-back-the-sun -p /content

!unzip \*.zip

from PIL import Image # used for loading images
import numpy as np
import os # used for navigating to image path
import imageio # used for writing images
import io
import pandas as pd
from google.colab import files
import keras
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.utils import to_categorical
from keras.preprocessing import image
from sklearn.metrics import confusion_matrix,precision_score
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from keras.regularizers import l2

"""IMPORTS FOR KERAS PANDAS AND MATPLOTLIB"""

#read through training data using pandas
data2 = pd.read_csv('additional_training.csv')
data = pd.read_csv('training.csv')
# the following code concatenates the 2 files and replaces NaN values with 0
with open('training.csv') as e:
    with open('additional_training.csv', 'r') as f:
        with open('new_additional_training.csv', 'w') as f1:
            for line in e:
                f1.write(line)
            next(f)
            for line in f:
                f1.write(line)
            



data3 = pd.read_csv('new_additional_training.csv')
data3 = data3.fillna(0)
merged = data.append(data3,sort =False)
#create x and y data sets
x= data3.drop(columns=['prediction'])
y= data3['prediction']

"""in this section we import the keras utilities and prepare our data by creating a data set and validation set in the form of x and y, y being the validation set; I have also concatenated the two training sets into a csv that replaces al NaN values with 0."""

#train test split for x and y
x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.10,random_state=0)
x_train.shape,y_train.shape,x_test.shape,y_test.shape

"""now we create our train test split with the parameters test size specifying the amount of data that will be used for tesing and random state that specifies whether the train and test subsets are divided randomly."""

#instantiate sequential neural network
model = Sequential()
#input layer
model.add(Dense(100,activation='relu',input_dim=4609,kernel_regularizer=l2(0.1)))
#hidden layer 1
model.add(Dropout(0.3, noise_shape=None,seed=None))
#hidden layer 2
model.add(Dense(100,activation='relu',kernel_regularizer=l2(0.1)))
#hidden layer 3
model.add(Dropout(0.3, noise_shape=None,seed=None))
#output layer 
model.add(Dense(1,activation='sigmoid'))

#compile model using the adaptive moment estimation optimization
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

"""here we compile the model; using binary crossentropy (output of values whose probability lies between 0 and 1) and use accuracy as our metric for each epoch."""

model_output = model.fit(x_train,y_train,epochs=200,batch_size=2000,verbose=1,validation_data=(x_test,y_test))

"""this is our fitting using the x y data sets that we initialised earlier; it carres over 500 epochs (the number of times our model goes through the data being being equal to 500) with a batch size of 2000(the number of training samples used before the model updates being equal to 2000). I have also set verbose to 1 such that I can see the individual metric of each epoch"""

import matplotlib.pyplot as plt
#plot accuracy
plt.plot(model_output.history['acc'])
plt.plot(model_output.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.savefig("accuracy.png")
plt.show()
#plot loss
plt.plot(model_output.history['loss'])
plt.plot(model_output.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.savefig("loss.png")
plt.show()

"""we plot the metrics for our fitting (val_acc and loss)"""

#test accuracy of model
y_pred = model.predict(x_test)
rounded = [round(x[0]) for x in y_pred]
y_pred1 = np.array(rounded,dtype='int64')
confusion_matrix(y_test,y_pred1)
print(y_pred1)
print('model accuracy:',precision_score(y_test,y_pred1))

"""Internal tesing of our models accuracy"""

#load testing csv
test_data = pd.read_csv('testing.csv')
u= test_data
v= test_data
print(len(u))

#make predictions for test file
import numpy

v_pred = model.predict(u)
for w in range(len(u)-1):
    rounded = [round(u[0]) for u in v_pred]
    v_pred1 = np.array(rounded,dtype='int64')


print(len(v_pred1))
z = len(v_pred1)
print (v_pred1)

import csv


#create valid submission csv by writing each prediction against the length of array
with open('my submission.csv', mode='w') as csvfile:
    filewriter = csv.writer(csvfile, delimiter=',',
                            quotechar='|', quoting=csv.QUOTE_MINIMAL)
    filewriter.writerow(['ID', 'prediction'])
    for z in range(len(v_pred1)):
        filewriter.writerow([z+1, v_pred1.item(z)])

files.download("my submission.csv")